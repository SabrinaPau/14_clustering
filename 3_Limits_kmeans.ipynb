{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms compared: K-Means and DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to introduce a second clustering algorithm, **DBSCAN**.\n",
    "\n",
    "In this notebook we will use an artificially generated dataset to apply and compare these two clustering algorithms. We will visualize the results of the algorithms to evaluate their statistical performance and also compare the run times.\n",
    "\n",
    "At the end of this notebook you should: \n",
    "* Have a rough overview of the advantages and disadvantages of the algorithms\n",
    "* Have an idea how both algorithms work\n",
    "* Know which hyperparamter you can use to adjust the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import time\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specifing figure layout\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_color_codes('bright')\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare different clustering algorithms we'll need some data. Since most of the time real-world data has more than two features/dimensions it's just not possible to visualize it. For our comparison we will therefore use an artifical created dataset with some non-globular clusters and noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data.zip file\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('data/cluster_data.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "plt.scatter(data.T[0], data.T[1], c='b', **plot_kwds)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely identify some clusters even if the data is quite noisy in some parts. To make things more interesting, some clusters are clearly elongated and drawing clear boundaries is difficult. Let's see how the different algorithms will perform when clustering the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Algorithms\n",
    "\n",
    "Hopefully you remember the accronym DRY. We don't want to write the same code for testing the different algorithms over and over again. Therefore we'll define a utility function that does the clustering and takes the time the algorithm needs as well as visualizes the result.\n",
    "\n",
    "We need to keep in mind that in real use cases we usually **cannot** look at the data to see if some of the points were assigned to a wrong cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# defining a utility function for testing the clustering algorithms\n",
    "def plot_clusters(data, algorithm, args, kwds):\n",
    "    # cluster the data while taking the time the process needs \n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # defining the colors for visual representation\n",
    "    palette = sns.color_palette('bright', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    \n",
    "    # plotting the data, removing the axis and adding title and time \n",
    "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n",
    "    plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-Means is probably the most common clustering algorithm. It is fast, easy to understand and you will find an implementation in most of the statistical or machine learning tools you will come across to use at some point.\n",
    "\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "The objective of K-Means is simple: group similar data points together to a fixed number (*k*) of clusters. Besides the data the only thing the algorithm needs as an input is the number of clusters *k*. *K* in this case can also be seen as the number of centroids, with a centroid being the imaginary or real location representing the cluster center.\n",
    "\n",
    "The algorithm aims to choose centroids that minimise the **inertia**, or **within-cluster sum of squared criterion**:\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_j - \\mu_i||^2)$$\n",
    "\n",
    "It starts with an initial estimate of the centroid locations. The algorithm then iterates between two steps: \n",
    "\n",
    "#### 1. Data assignment\n",
    "\n",
    "Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, based on the squared Euclidean distance. More formally, if $c_i$ is the collection of centroids in set $C$, then each data point $x$ is assigned to a cluster based on\n",
    "\n",
    "$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n",
    "where dist( Â· ) is the standard ($L_2$) Euclidean distance. Let the set of data point assignments for each ith cluster centroid be $S_i$.\n",
    "\n",
    "\n",
    "#### 2. Centroid update       \n",
    "\n",
    "In this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster.\n",
    "\n",
    "$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i x_i}$$\n",
    "\n",
    "The difference between the old and new centroids are computed and the algorithm repeats the two steps until a stopping criteria is met (i.e., the value is less than a threshold, no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n",
    "\n",
    "\n",
    "#### Convergence and random initialization\n",
    "\n",
    "Given enough time this algorithm is guaranteed to converge to a result. The result may be a local minimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "* Inertia makes the assumption that clusters are convex and isotropic, which often is not the case. So the algorithm has problems identifying elongated clusters or manifolds with irregular shapes.\n",
    "* We need to specify how many clusters we expect. If we don't know which number might be reasonable we have to run the algorithm several times with different *k* values and evaluate each run using for example the variation on intra-cluster vs. inter-cluster distance to find the optimal value for *k*.  \n",
    "\n",
    "#### Advantages\n",
    "\n",
    "* K-Means can be exceptionally efficient. This is K-Means big win. It's a simple algorithm and with the right tricks and optimizations there are few algorithms that can compete with K-Means for performance. If you have truly huge data then K-Means might be your only option.\n",
    "\n",
    "\n",
    "Let's see how the algorithm performs with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.KMeans, (), {'n_clusters':6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the limitations of K-Means in mind the result is not surprising. The assumption of perfectly globular clusters means that the real rather elongated clusters have been poorly identified. What is even worse, the noise points get lumped into clusters as well. In some cases, depending on where the cluster center end up, even relatively distant points are still assigned to a cluster. \n",
    "\n",
    "To say something positive as well: K-Means was really quick clustering the data. So at least it's a fast no.\n",
    "\n",
    "If you want to try out how K-Means performs on different data have a look at this [website](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) by Naftali Harris. You can choose from a range of different datasets and also decide if you want to set the centroids manually or assign them randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "DBSCAN is a density based algorithm. It views clusters as areas of high density separated by areas of low density.\n",
    "\n",
    "It is also the first actual clustering algorithm we've tested so far: it doesn't need to assign every point to a cluster but really extracts the *dense* clusters and leaves the sparse background/noise untouched. K-Means was rather partitioning the data than clustering it. \n",
    "\n",
    "Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples).\n",
    "\n",
    "The most important hyperparameter we have to specify matching our data is `eps`. Without a proper `eps` DBSCAN will most likely produce unsatisfactory results. With `eps` we can change the maximum distance between samples to be considered in the neighborhood of one another. In order to visualize the effects of `eps` on the clustering performance have a look at this [awesome website](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) and try out different values. \n",
    "\n",
    "\n",
    "![dbscan](https://3.bp.blogspot.com/-rDYuyg00Z0w/WXA-OQpkAfI/AAAAAAAAI_I/QshfNVNHD_wXJwXEipRIVzDSX5iOEAy2wCEwYBhgL/s1600/DBSCAN_Points.PNG)\n",
    "\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "* Clusters with varying density can cause problems.\n",
    "  \n",
    "\n",
    "#### Advantages\n",
    "\n",
    "* Clusters don't need to be globular.\n",
    "* Noise will not be assigned to a cluster.\n",
    "* It is also stable accross runs and will generate reproducible results. \n",
    "* Its very fast and can cluster very large datasets.\n",
    "\n",
    "Let's check how DBSCAN performs on our data. We choose `eps` to be 0.025 (there is no way to calculate the right value for epsilon, you have to try which works best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_clusters(data, cluster.DBSCAN, (), {'eps':0.025})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty decent. The algorthm identified several natural clusters (also the elongated ones) and didn't assign additional noise to those clusters. It didn't perform so well on the rather sparse cluster and defined some tiny additional clusters. That's due to its limited abitilty to handle variable density clusters. All in all the algorithm is doing a decent job."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9196ed29717c032a5647b3d4e686475ff2d81ecd1a2edd58fd17f587842f5f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('clustering_exercise')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
